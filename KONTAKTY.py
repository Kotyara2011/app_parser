import re
import io
import time
import queue
import concurrent.futures
import urllib.parse as urlparse
from dataclasses import dataclass, field
from typing import List, Set, Dict, Tuple, Optional
import zipfile

import requests
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st

# ==========================
# ---------- Utils ---------
# ==========================

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
)

HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ru,en;q=0.9"}
DEFAULT_TIMEOUT = 15

CONTACT_PATH_GUESSES = [
    "/", "/contact", "/contacts", "/kontakty", "/kontact", "/o-kompanii",
    "/about", "/company", "/rekvizity", "/requisites", "/support"
]

# —Ç–æ–ª—å–∫–æ —Ç–µ–ª–µ—Ñ–æ–Ω—ã –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å +
PHONE_RE = re.compile(r"\+\d[\d\s\-()]{8,}\d")
EMAIL_RE = re.compile(r"[a-zA-Z0-9_.+\-]+@[a-zA-Z0-9\-]+\.[a-zA-Z0-9\-.]+")
INN_CANDIDATE_RE = re.compile(r"(?<!\d)(\d{10}|\d{12})(?!\d)")
INN_NEAR_RE = re.compile(r"–ò–ù–ù\s*[:‚Ññ#\-]*\s*(\d{10}|\d{12})", re.IGNORECASE)

# ---------- INN validation ----------

def _checksum(digits: List[int], coeffs: List[int]) -> int:
    s = sum(d * c for d, c in zip(digits, coeffs))
    return (s % 11) % 10


def validate_inn(inn: str) -> bool:
    if not inn.isdigit():
        return False
    if len(inn) == 10:
        coeffs10 = [2, 4, 10, 3, 5, 9, 4, 6, 8]
        d = [int(x) for x in inn]
        return _checksum(d[:9], coeffs10) == d[9]
    if len(inn) == 12:
        coeffs11 = [7, 2, 4, 10, 3, 5, 9, 4, 6, 8]
        coeffs12 = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]
        d = [int(x) for x in inn]
        ctrl11 = _checksum(d[:10], coeffs11)
        ctrl12 = _checksum(d[:11], coeffs12)
        return ctrl11 == d[10] and ctrl12 == d[11]
    return False

# ---------- Helpers for domain normalization ----------

DATE_RE_SIMPLE = re.compile(r"^\d{4}-\d{2}-\d{2}$")
DOMAIN_LIKE_RE = re.compile(r"^(?:https?://)?(?:www\.)?([A-Za-z0-9\-]{1,63}(?:\.[A-Za-z0-9\-]{1,63})+)(?:[:/].*)?$")


def looks_like_date(s: str) -> bool:
    if not isinstance(s, str):
        return False
    s = s.strip()
    if not s:
        return False
    if DATE_RE_SIMPLE.match(s):
        return True
    try:
        ts = pd.to_datetime(s, errors='coerce')
        return not pd.isna(ts)
    except Exception:
        return False


def normalize_domain(raw: object) -> Optional[str]:
    if raw is None:
        return None
    if hasattr(raw, 'tzinfo') or isinstance(raw, (pd.Timestamp,)):
        return None

    s = str(raw).strip()
    if not s:
        return None

    if s == ';' or (';' in s and len(s.strip(';').strip()) == 0):
        return None
    if looks_like_date(s):
        return None

    parts = re.split(r"[;,\s]+", s)
    for part in parts:
        token = part.strip().strip('"\'')
        if not token or token == ';':
            continue
        if '@' in token:
            continue
        m = DOMAIN_LIKE_RE.match(token)
        host = None
        if m:
            host = m.group(1)
        else:
            try:
                p = urlparse.urlparse(token if token.startswith('http') else 'https://' + token)
                if p.netloc:
                    host = p.netloc
                else:
                    host = token
            except Exception:
                host = None
        if host:
            host = host.split('@')[-1].split(':')[0].lower()
            if '.' in host and not host.replace('.', '').isdigit():
                if host.startswith('www.'):
                    host = host[4:]
                return f"https://{host}"
    return None


def urljoin_keep(base: str, path: str) -> str:
    return urlparse.urljoin(base if base.endswith('/') else base + '/', path)

# ---------- Fetching ----------

def fetch(url: str, timeout: int = DEFAULT_TIMEOUT) -> Optional[str]:
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)
        ct = r.headers.get('content-type', '')
        if r.status_code >= 200 and r.status_code < 300 and ('text' in ct or 'html' in ct):
            return r.text[:2_000_000]
    except Exception:
        return None
    return None

# ---------- Extraction ----------

def clean_phone(p: str) -> str:
    s = re.sub(r"[^\d+]+", "", p)
    s = re.sub(r"^\++", "+", s)
    if not s.startswith("+"):
        return ""
    return s


def extract_contacts(html: str) -> Tuple[Set[str], Set[str], Set[str]]:
    phones: Set[str] = set()
    emails: Set[str] = set()
    inns: Set[str] = set()

    for m in EMAIL_RE.finditer(html):
        emails.add(m.group(0).lower())

    for m in PHONE_RE.finditer(html):
        ph = clean_phone(m.group(0))
        if ph:
            phones.add(ph)

    for m in INN_NEAR_RE.finditer(html):
        cand = m.group(1)
        if validate_inn(cand):
            inns.add(cand)

    for m in INN_CANDIDATE_RE.finditer(html):
        cand = m.group(1)
        if validate_inn(cand):
            inns.add(cand)

    return phones, emails, inns

# ---------- Crawl per domain ----------

@dataclass
class CrawlResult:
    domain: str
    page_hits: List[Dict[str, object]] = field(default_factory=list)

    def aggregate(self) -> Dict[str, object]:
        agg_phones: Set[str] = set()
        agg_emails: Set[str] = set()
        agg_inns: Set[str] = set()
        for row in self.page_hits:
            agg_phones.update(row.get('phones', set()))
            agg_emails.update(row.get('emails', set()))
            agg_inns.update(row.get('inns', set()))
        return {
            "–î–æ–º–µ–Ω": self.domain,
            "–¢–µ–ª–µ—Ñ–æ–Ω—ã": ", ".join(sorted(agg_phones)) if agg_phones else "",
            "–ü–æ—á—Ç—ã": ", ".join(sorted(agg_emails)) if agg_emails else "",
            "–ò–ù–ù": ", ".join(sorted(agg_inns)) if agg_inns else "",
        }


def same_host(url: str, base: str) -> bool:
    try:
        p1 = urlparse.urlparse(url)
        p2 = urlparse.urlparse(base)
        return p1.netloc.lower().lstrip("www.") == p2.netloc.lower().lstrip("www.")
    except Exception:
        return False


def crawl_domain(base: str, max_pages: int = 15, timeout: int = DEFAULT_TIMEOUT) -> CrawlResult:
    seen: Set[str] = set()
    q: "queue.Queue[str]" = queue.Queue()

    seeded = []
    for path in CONTACT_PATH_GUESSES:
        seeded.append(urljoin_keep(base, path))
    for u in seeded:
        q.put(u)

    result = CrawlResult(domain=base)

    while not q.empty() and len(seen) < max_pages:
        url = q.get()
        if url in seen:
            continue
        seen.add(url)

        html = fetch(url, timeout=timeout)
        if not html:
            continue

        phones, emails, inns = extract_contacts(html)
        if phones or emails or inns:
            result.page_hits.append({
                "url": url,
                "phones": phones,
                "emails": emails,
                "inns": inns,
            })

        try:
            soup = BeautifulSoup(html, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a["href"].strip()
                new_url = urljoin_keep(url, href)
                if same_host(new_url, base) and new_url not in seen and new_url.startswith("http"):
                    if any(x in new_url.lower() for x in ["contact", "kont", "about", "company", "rekviz"]):
                        q.put(new_url)
        except Exception:
            pass

    return result

# ==========================
# ---------- Streamlit UI -
# ==========================

st.set_page_config(page_title="–ü–∞—Ä—Å–µ—Ä –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –ø–æ –¥–æ–º–µ–Ω–∞–º", page_icon="üìá", layout="wide")

st.title("üìá –ü–∞—Ä—Å–µ—Ä –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: –¢–µ–ª–µ—Ñ–æ–Ω—ã / Email / –ò–ù–ù")
st.write("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel (.xlsx) –∏–ª–∏ CSV —Å–æ —Å–ø–∏—Å–∫–æ–º –¥–æ–º–µ–Ω–æ–≤ ‚Äî –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏–∑–≤–ª–µ—á—ë—Ç —Ç–æ–ª—å–∫–æ –¥–æ–º–µ–Ω—ã –∏ –æ–±–æ–π–¥–µ—Ç —Å–∞–π—Ç –ø–æ –Ω–∏–º, —Å–æ–±–∏—Ä–∞—è —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –ø–æ—á—Ç—ã –∏ –ò–ù–ù.")

with st.sidebar:
    st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    max_pages = st.slider("–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –¥–æ–º–µ–Ω", 5, 50, 15, step=1,
                          help="–õ–∏–º–∏—Ç –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ª–≥–∏–π –∏ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥.")
    timeout = st.slider("–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (—Å–µ–∫)", 5, 60, DEFAULT_TIMEOUT, step=1)
    st.caption("*–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å–æ–±–ª—é–¥–∞–µ—Ç –≤–µ–∂–ª–∏–≤—ã–π —Ä–µ–∂–∏–º: –Ω–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –¥–æ–º–µ–Ω–∞ –∏ —á–∏—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.*")

uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel (.xlsx) –∏–ª–∏ CSV (.csv)", type=["xlsx", "csv"]) 

if uploaded:
    try:
        if uploaded.name.lower().endswith('.csv'):
            try:
                df_in = pd.read_csv(uploaded, sep=None, engine='python')
            except Exception:
                uploaded.seek(0)
                df_in = pd.read_csv(uploaded, delimiter=';', engine='python')
        else:
            df_in = pd.read_excel(uploaded)
    except Exception as e:
        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")
        st.stop()

    st.success(f"–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω: {df_in.shape[0]} —Å—Ç—Ä–æ–∫, {df_in.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")

    domains_set: Set[str] = set()
    for col in df_in.columns:
        for val in df_in[col].dropna().astype(str).tolist():
            d = normalize_domain(val)
            if d:
                domains_set.add(d)

    if not domains_set:
        st.warning("–ê–≤—Ç–æ-–¥–µ—Ç–µ–∫—Ç –¥–æ–º–µ–Ω–æ–≤ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É –≤—Ä—É—á–Ω—É—é.")
        col_choice = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É —Å –¥–æ–º–µ–Ω–∞–º–∏ (–∏–ª–∏ –æ—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç–æ–π)", options=[""] + list(df_in.columns))
        if col_choice:
            for val in df_in[col_choice].dropna().astype(str).tolist():
                d = normalize_domain(val)
                if d:
                    domains_set.add(d)

    domains = sorted(domains_set)
    st.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(domains)}")
    if len(domains) > 0:
        st.write("–ü—Ä–∏–º–µ—Ä—ã –¥–æ–º–µ–Ω–æ–≤:", domains[:50])

if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–±–æ—Ä", type="primary"):
    progress = st.progress(0)
    status = st.empty()

    all_results: List[CrawlResult] = []
    records: List[Dict[str, object]] = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_dom = {
            executor.submit(crawl_domain, dom, max_pages=max_pages, timeout=int(timeout)): dom
            for dom in domains
        }

        for i, future in enumerate(concurrent.futures.as_completed(future_to_dom), start=1):
            dom = future_to_dom[future]
            status.write(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {dom} ({i}/{len(domains)}) ‚Ä¶")
            try:
                res = future.result()
                all_results.append(res)
                for hit in res.page_hits:
                    records.append({
                        "–î–æ–º–µ–Ω": dom,
                        "URL": hit["url"],
                        "–¢–µ–ª–µ—Ñ–æ–Ω—ã": ", ".join(sorted(hit["phones"])) if hit["phones"] else "",
                        "–ü–æ—á—Ç—ã": ", ".join(sorted(hit["emails"])) if hit["emails"] else "",
                        "–ò–ù–ù": ", ".join(sorted(hit["inns"])) if hit["inns"] else "",
                    })
            except Exception as e:
                records.append({
                    "–î–æ–º–µ–Ω": dom,
                    "URL": "",
                    "–¢–µ–ª–µ—Ñ–æ–Ω—ã": "",
                    "–ü–æ—á—Ç—ã": "",
                    "–ò–ù–ù": f"–û—à–∏–±–∫–∞: {e}",
                })
            progress.progress(i / max(1, len(domains)))
            time.sleep(0.02)

        summary_rows = [r.aggregate() for r in all_results]
        df_summary = pd.DataFrame(summary_rows)
        df_records = pd.DataFrame(records)

        st.subheader("–ò—Ç–æ–≥ ‚Äî —Å–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º")
        st.dataframe(df_summary, use_container_width=True)

        st.subheader("–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º")
        st.dataframe(df_records, use_container_width=True)

        output_xlsx = io.BytesIO()
        with pd.ExcelWriter(output_xlsx, engine="openpyxl") as writer:
            df_summary.to_excel(writer, index=False, sheet_name="–°–≤–æ–¥–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º")
            df_records.to_excel(writer, index=False, sheet_name="–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏")
        output_xlsx.seek(0)

        st.download_button(
            label="üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç (output.xlsx)",
            data=output_xlsx,
            file_name="output.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

        output_zip = io.BytesIO()
        with zipfile.ZipFile(output_zip, mode='w') as zf:
            zf.writestr('summary.csv', df_summary.to_csv(index=False, encoding='utf-8-sig'))
            zf.writestr('records.csv', df_records.to_csv(index=False, encoding='utf-8-sig'))
        output_zip.seek(0)

        st.download_button(
            label="üíæ –°–∫–∞—á–∞—Ç—å CSV (summary.csv + records.csv) –≤ ZIP",
            data=output_zip,
            file_name="output_csvs.zip",
            mime="application/zip",
        )

        st.success("–ì–æ—Ç–æ–≤–æ! –ú–æ–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
else:
    st.caption("–û–∂–∏–¥–∞—é –∑–∞–≥—Ä—É–∑–∫—É —Ñ–∞–π–ª–∞ (.xlsx –∏–ª–∏ .csv)‚Ä¶")

st.markdown("""
---
**–ò–∑–º–µ–Ω–µ–Ω–∏—è –∏ –≤–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã:**
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: XLSX –∏ CSV.
- –ò–∑ —è—á–µ–µ–∫ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è **—Ç–æ–ª—å–∫–æ –¥–æ–º–µ–Ω—ã**: –¥–∞—Ç—ã (–Ω–∞–ø—Ä. `2025-08-26`) –∏ –æ–¥–∏–Ω–æ—á–Ω—ã–µ `;` **–∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è**.
- –ï—Å–ª–∏ –≤ —è—á–µ–π–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π ‚Äî —Å—Ç—Ä–æ–∫–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç—Å—è –ø–æ `,`, `;` –∏ –ø—Ä–æ–±–µ–ª–∞–º, –∏ –ø—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –¥–æ–º–µ–Ω –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.
- –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –¥–≤—É—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö: `output.xlsx` (2 –ª–∏—Å—Ç–∞) –∏ ZIP —Å –¥–≤—É–º—è CSV: `summary.csv` –∏ `records.csv`.
- –£–±—Ä–∞–Ω –ø—Ä—è–º–æ–π `global DEFAULT_TIMEOUT`: —Ç–∞–π–º–∞—É—Ç –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä –≤ –æ–±—Ö–æ–¥—â–∏–∫–µ.
- ‚ö°Ô∏è –¢–µ–ø–µ—Ä—å –≤ –±–∞–∑—É –ø–æ–ø–∞–¥–∞—é—Ç —Ç–æ–ª—å–∫–æ —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å `+`.
""")
