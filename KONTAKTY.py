import re
import io
import time
import queue
import concurrent.futures
import urllib.parse as urlparse
from dataclasses import dataclass, field
from typing import List, Set, Dict, Tuple, Optional
import zipfile

import requests
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st

# ==========================
# ---------- Utils ---------
# ==========================

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
)
HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ru,en;q=0.9"}
DEFAULT_TIMEOUT = 15

CONTACT_PATH_GUESSES = [
    "/", "/contact", "/contacts", "/kontakty", "/kontact",
    "/o-kompanii", "/about", "/company", "/rekvizity", "/requisites", "/support"
]

PHONE_RE = re.compile(r"\+?\d[\d\s\-()]{8,}\d")
EMAIL_RE = re.compile(r"[a-zA-Z0-9_.+\-]+@[a-zA-Z0-9\-]+\.[a-zA-Z0-9\-.]+")
INN_CANDIDATE_RE = re.compile(r"(?<!\d)(\d{10}|\d{12})(?!\d)")
INN_NEAR_RE = re.compile(r"Ð˜ÐÐ\s*[:â„–#\-]*\s*(\d{10}|\d{12})", re.IGNORECASE)


# ---------- INN validation ----------
def _checksum(digits: List[int], coeffs: List[int]) -> int:
    s = sum(d * c for d, c in zip(digits, coeffs))
    return (s % 11) % 10

def validate_inn(inn: str) -> bool:
    if not inn.isdigit():
        return False
    if len(inn) == 10:
        coeffs10 = [2, 4, 10, 3, 5, 9, 4, 6, 8]
        d = [int(x) for x in inn]
        return _checksum(d[:9], coeffs10) == d[9]
    if len(inn) == 12:
        coeffs11 = [7, 2, 4, 10, 3, 5, 9, 4, 6, 8]
        coeffs12 = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]
        d = [int(x) for x in inn]
        ctrl11 = _checksum(d[:10], coeffs11)
        ctrl12 = _checksum(d[:11], coeffs12)
        return ctrl11 == d[10] and ctrl12 == d[11]
    return False


# ---------- Helpers ----------
DATE_RE_SIMPLE = re.compile(r"^\d{4}-\d{2}-\d{2}$")
DOMAIN_LIKE_RE = re.compile(
    r"^(?:https?://)?(?:www\.)?([A-Za-z0-9\-]{1,63}(?:\.[A-Za-z0-9\-]{1,63})+)(?:[:/].*)?$"
)

def looks_like_date(s: str) -> bool:
    if not isinstance(s, str):
        return False
    s = s.strip()
    if not s:
        return False
    if DATE_RE_SIMPLE.match(s):
        return True
    try:
        ts = pd.to_datetime(s, errors='coerce')
        return not pd.isna(ts)
    except Exception:
        return False

def normalize_domain(raw: object) -> Optional[str]:
    if raw is None:
        return None
    if hasattr(raw, 'tzinfo') or isinstance(raw, (pd.Timestamp,)):
        return None
    s = str(raw).strip()
    if not s:
        return None
    if s == ';' or (';' in s and len(s.strip(';').strip()) == 0):
        return None
    if looks_like_date(s):
        return None

    parts = re.split(r"[;,\s]+", s)
    for part in parts:
        token = part.strip().strip('"\'')
        if not token or token == ';':
            continue
        if '@' in token:
            continue
        m = DOMAIN_LIKE_RE.match(token)
        host = None
        if m:
            host = m.group(1)
        else:
            try:
                p = urlparse.urlparse(token if token.startswith('http') else 'https://' + token)
                if p.netloc:
                    host = p.netloc
                else:
                    host = token
            except Exception:
                host = None
        if host:
            host = host.split('@')[-1].split(':')[0].lower()
            if '.' in host and not host.replace('.', '').isdigit():
                if host.startswith('www.'):
                    host = host[4:]
                return f"https://{host}"
    return None

def urljoin_keep(base: str, path: str) -> str:
    return urlparse.urljoin(base if base.endswith('/') else base + '/', path)


# ---------- Fetching ----------
def fetch(url: str, timeout: int = DEFAULT_TIMEOUT) -> Optional[str]:
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)
        ct = r.headers.get('content-type', '')
        if r.status_code >= 200 and r.status_code < 300 and ('text' in ct or 'html' in ct):
            return r.text[:2_000_000]
    except Exception:
        return None
    return None


# ---------- Extraction ----------
def clean_phone(p: str) -> Optional[str]:
    s = re.sub(r"[^\d+]+", "", p)
    s = re.sub(r"^\++", "+", s)
    if not s.startswith("+"):
        return None
    return s

def extract_contacts(html: str) -> Tuple[Set[str], Set[str], Set[str]]:
    phones: Set[str] = set()
    emails: Set[str] = set()
    inns: Set[str] = set()

    for m in EMAIL_RE.finditer(html):
        emails.add(m.group(0).lower())

    for m in PHONE_RE.finditer(html):
        ph = clean_phone(m.group(0))
        if ph:
            phones.add(ph)

    for m in INN_NEAR_RE.finditer(html):
        cand = m.group(1)
        if validate_inn(cand):
            inns.add(cand)

    for m in INN_CANDIDATE_RE.finditer(html):
        cand = m.group(1)
        if validate_inn(cand):
            inns.add(cand)

    return phones, emails, inns


# ---------- Crawl per domain ----------
@dataclass
class CrawlResult:
    domain: str
    page_hits: List[Dict[str, object]] = field(default_factory=list)

    def aggregate(self) -> Dict[str, object]:
        agg_phones: Set[str] = set()
        agg_emails: Set[str] = set()
        agg_inns: Set[str] = set()
        for row in self.page_hits:
            agg_phones.update(row.get('phones', set()))
            agg_emails.update(row.get('emails', set()))
            agg_inns.update(row.get('inns', set()))
        return {
            "Ð”Ð¾Ð¼ÐµÐ½": self.domain,
            "Ð¢ÐµÐ»ÐµÑ„Ð¾Ð½Ñ‹": ", ".join(sorted(agg_phones)) if agg_phones else "",
            "ÐŸÐ¾Ñ‡Ñ‚Ñ‹": ", ".join(sorted(agg_emails)) if agg_emails else "",
            "Ð˜ÐÐ": ", ".join(sorted(agg_inns)) if agg_inns else "",
        }

def same_host(url: str, base: str) -> bool:
    try:
        p1 = urlparse.urlparse(url)
        p2 = urlparse.urlparse(base)
        return p1.netloc.lower().lstrip("www.") == p2.netloc.lower().lstrip("www.")
    except Exception:
        return False

def crawl_domain(base: str, max_pages: int = 15, timeout: int = DEFAULT_TIMEOUT) -> CrawlResult:
    seen: Set[str] = set()
    q: "queue.Queue[str]" = queue.Queue()

    for path in CONTACT_PATH_GUESSES:
        q.put(urljoin_keep(base, path))

    result = CrawlResult(domain=base)

    while not q.empty() and len(seen) < max_pages:
        url = q.get()
        if url in seen:
            continue
        seen.add(url)

        html = fetch(url, timeout=timeout)
        if not html:
            continue

        phones, emails, inns = extract_contacts(html)
        if phones or emails or inns:
            result.page_hits.append({
                "url": url,
                "phones": phones,
                "emails": emails,
                "inns": inns,
            })

        try:
            soup = BeautifulSoup(html, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a["href"].strip()
                new_url = urljoin_keep(url, href)
                if same_host(new_url, base) and new_url not in seen and new_url.startswith("http"):
                    if any(x in new_url.lower() for x in ["contact", "kont", "about", "company", "rekviz"]):
                        q.put(new_url)
        except Exception:
            pass

    return result


# ==========================
# ---------- Streamlit UI --
# ==========================

st.set_page_config(page_title="ÐŸÐ°Ñ€ÑÐµÑ€ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¾Ð² Ð¿Ð¾ Ð´Ð¾Ð¼ÐµÐ½Ð°Ð¼", page_icon="ðŸ“‡", layout="wide")
st.title("ðŸ“‡ ÐŸÐ°Ñ€ÑÐµÑ€ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¾Ð²: Ð¢ÐµÐ»ÐµÑ„Ð¾Ð½Ñ‹ / Email / Ð˜ÐÐ")

st.write("Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Excel (.xlsx) Ð¸Ð»Ð¸ CSV ÑÐ¾ ÑÐ¿Ð¸ÑÐºÐ¾Ð¼ Ð´Ð¾Ð¼ÐµÐ½Ð¾Ð² â€” Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡Ñ‘Ñ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð¾Ð¼ÐµÐ½Ñ‹ Ð¸ Ð¾Ð±Ð¾Ð¹Ð´ÐµÑ‚ ÑÐ°Ð¹Ñ‚ Ð¿Ð¾ Ð½Ð¸Ð¼, ÑÐ¾Ð±Ð¸Ñ€Ð°Ñ Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½Ñ‹, Ð¿Ð¾Ñ‡Ñ‚Ñ‹ Ð¸ Ð˜ÐÐ.")

with st.sidebar:
    st.header("ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸")
    max_pages = st.slider("ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† Ð½Ð° Ð´Ð¾Ð¼ÐµÐ½", 5, 50, 15, step=1)
    timeout = st.slider("Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° (ÑÐµÐº)", 5, 60, DEFAULT_TIMEOUT, step=1)

    uploaded = st.file_uploader("Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Excel (.xlsx) Ð¸Ð»Ð¸ CSV (.csv)", type=["xlsx", "csv"])

if uploaded:
    try:
        if uploaded.name.lower().endswith('.csv'):
            try:
                df_in = pd.read_csv(uploaded, sep=None, engine='python')
            except Exception:
                uploaded.seek(0)
                df_in = pd.read_csv(uploaded, delimiter=';', engine='python')
        else:
            df_in = pd.read_excel(uploaded)
    except Exception as e:
        st.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»: {e}")
        st.stop()

    st.success(f"Ð¤Ð°Ð¹Ð» Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð½: {df_in.shape[0]} ÑÑ‚Ñ€Ð¾Ðº, {df_in.shape[1]} ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº")

    domains_set: Set[str] = set()
    for col in df_in.columns:
        for val in df_in[col].dropna().astype(str).tolist():
            d = normalize_domain(val)
            if d:
                domains_set.add(d)

    if not domains_set:
        st.warning("ÐÐ²Ñ‚Ð¾-Ð´ÐµÑ‚ÐµÐºÑ‚ Ð´Ð¾Ð¼ÐµÐ½Ð¾Ð² Ð½Ðµ Ð´Ð°Ð» Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð². Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ.")
        col_choice = st.selectbox("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ Ñ Ð´Ð¾Ð¼ÐµÐ½Ð°Ð¼Ð¸ (Ð¸Ð»Ð¸ Ð¾ÑÑ‚Ð°Ð²ÑŒÑ‚Ðµ Ð¿ÑƒÑÑ‚Ð¾Ð¹)", options=[""] + list(df_in.columns))
        if col_choice:
            for val in df_in[col_choice].dropna().astype(str).tolist():
                d = normalize_domain(val)
                if d:
                    domains_set.add(d)

    domains = sorted(domains_set)
    st.info(f"ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð´Ð¾Ð¼ÐµÐ½Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸: {len(domains)}")
    if domains:
        st.write("ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð´Ð¾Ð¼ÐµÐ½Ð¾Ð²:", domains[:50])

    if st.button("Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ ÑÐ±Ð¾Ñ€", type="primary"):
        progress = st.progress(0)
        status = st.empty()
        all_results: List[CrawlResult] = []
        records: List[Dict[str, object]] = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            future_to_dom = {executor.submit(crawl_domain, dom, max_pages=max_pages, timeout=int(timeout)): dom for dom in domains}
            for i, future in enumerate(concurrent.futures.as_completed(future_to_dom), start=1):
                dom = future_to_dom[future]
                status.write(f"ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ {dom} ({i}/{len(domains)}) â€¦")
                try:
                    res = future.result()
                    all_results.append(res)
                    for hit in res.page_hits:
                        records.append({
                            "Ð”Ð¾Ð¼ÐµÐ½": dom,
                            "URL": hit["url"],
                            "Ð¢ÐµÐ»ÐµÑ„Ð¾Ð½Ñ‹": ", ".join(sorted(hit["phones"])) if hit["phones"] else "",
                            "ÐŸÐ¾Ñ‡Ñ‚Ñ‹": ", ".join(sorted(hit["emails"])) if hit["emails"] else "",
                            "Ð˜ÐÐ": ", ".join(sorted(hit["inns"])) if hit["inns"] else "",
                        })
                except Exception as e:
                    records.append({
                        "Ð”Ð¾Ð¼ÐµÐ½": dom,
                        "URL": "",
                        "Ð¢ÐµÐ»ÐµÑ„Ð¾Ð½Ñ‹": "",
                        "ÐŸÐ¾Ñ‡Ñ‚Ñ‹": "",
                        "Ð˜ÐÐ": f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}",
                    })
                progress.progress(i / max(1, len(domains)))
                time.sleep(0.02)

        summary_rows = [r.aggregate() for r in all_results]
        df_summary = pd.DataFrame(summary_rows)
        df_records = pd.DataFrame(records)

        st.subheader("Ð˜Ñ‚Ð¾Ð³ â€” ÑÐ²Ð¾Ð´Ð½Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð¿Ð¾ Ð´Ð¾Ð¼ÐµÐ½Ð°Ð¼")
        st.dataframe(df_summary, use_container_width=True)

        st.subheader("ÐÐ°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¿Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°Ð¼")
        st.dataframe(df_records, use_container_width=True)

        # Save XLSX
        output_xlsx = io.BytesIO()
        with pd.ExcelWriter(output_xlsx, engine="openpyxl") as writer:
            df_summary.to_excel(writer, index=False, sheet_name="Ð¡Ð²Ð¾Ð´ÐºÐ° Ð¿Ð¾ Ð´Ð¾Ð¼ÐµÐ½Ð°Ð¼")
            df_records.to_excel(writer, index=False, sheet_name="ÐÐ°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸")
        output_xlsx.seek(0)
        st.download_button(
            label="ðŸ’¾ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ (output.xlsx)",
            data=output_xlsx,
            file_name="output.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

        # Save CSV ZIP
        output_zip = io.BytesIO()
        with zipfile.ZipFile(output_zip, mode='w') as zf:
            zf.writestr('summary.csv', df_summary.to_csv(index=False, encoding='utf-8-sig'))
            zf.writestr('records.csv', df_records.to_csv(index=False, encoding='utf-8-sig'))
        output_zip.seek(0)
        st.download_button(
            label="ðŸ’¾ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ CSV (summary.csv + records.csv) Ð² ZIP",
            data=output_zip,
            file_name="output_csvs.zip",
            mime="application/zip",
        )

        st.success("Ð“Ð¾Ñ‚Ð¾Ð²Ð¾! ÐœÐ¾Ð¶Ð½Ð¾ ÑÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚.")
else:
    st.caption("ÐžÐ¶Ð¸Ð´Ð°ÑŽ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ Ñ„Ð°Ð¹Ð»Ð° (.xlsx Ð¸Ð»Ð¸ .csv)â€¦")
